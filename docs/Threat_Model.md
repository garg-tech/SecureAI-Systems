# Threat Modeling for MNIST CNN Classification System
## STRIDE Framework Analysis

---

## System Overview

**System**: MNIST Handwritten Digit Classification CNN  
**Purpose**: Classify 28Ã—28 pixel grayscale images of handwritten digits (0-9)  
**Architecture**: PyTorch-based Convolutional Neural Network  
**Deployment**: Research/Educational Environment  

---

## System Components and Data Flow

```
[Input Images] â†’ [Preprocessing] â†’ [CNN Model] â†’ [Classification] â†’ [Output/Decision]
       â†‘               â†‘              â†‘              â†‘              â†‘
   [Training Data] â†’ [Training] â†’ [Model Weights] â†’ [Inference] â†’ [Results]
```

### Key Assets
1. **Training Dataset** (MNIST 60,000 samples)
2. **Model Architecture** (CNN with ~1.2M parameters)
3. **Trained Model Weights** (mnist_custom_cnn.pth)
4. **Classification Predictions**
5. **Training Infrastructure**

---

## STRIDE Threat Analysis

### ğŸ­ **S - Spoofing**

#### Threat: Adversarial Examples Impersonating Legitimate Inputs
- **Description**: Maliciously crafted inputs that appear legitimate but cause misclassification
- **Attack Vector**: FGSM, PGD, C&W attacks with imperceptible perturbations
- **Impact**: HIGH - 55.9% attack success rate demonstrated
- **Likelihood**: HIGH - Easy to generate with gradient-based methods
- **Evidence**: Baseline model accuracy drops from 98.98% to 43.65% under FGSM attack

**Specific Scenarios:**
1. **Input Spoofing**: Adversarial digits designed to fool the classifier
2. **Model Mimicking**: Adversarial examples transferable across similar models
3. **Real-world Spoofing**: Physical adversarial patches on handwritten digits

**Mitigations:**
- âœ… **Adversarial Training**: Implemented, improving robustness to 95.02%
- âš ï¸ **Input Validation**: Basic normalization implemented
- âŒ **Adversarial Detection**: Not implemented
- âŒ **Ensemble Methods**: Not implemented

#### Threat: Model Identity Spoofing
- **Description**: Loading malicious models disguised as legitimate ones
- **Attack Vector**: Replacing model files with backdoored versions
- **Impact**: CRITICAL - Complete system compromise
- **Likelihood**: MEDIUM - Requires access to model storage
- **Evidence**: SAST analysis found unsafe `torch.load()` usage

---

### ğŸ”§ **T - Tampering**

#### Threat: Training Data Poisoning
- **Description**: Injection of malicious samples into training dataset
- **Attack Vector**: Backdoor triggers, label flipping, feature manipulation
- **Impact**: HIGH - Successful backdoor implantation demonstrated
- **Likelihood**: MEDIUM - Requires access to training pipeline
- **Evidence**: 100% backdoor success with only 100 poisoned samples (0.17% of dataset)

**Specific Scenarios:**
1. **Trigger-Based Poisoning**: 3Ã—3 white square trigger causing misclassification
2. **Label Flipping**: Changing correct labels during training
3. **Feature Space Poisoning**: Subtle modifications to input features

**Mitigations:**
- âŒ **Data Integrity Verification**: Not implemented
- âŒ **Anomaly Detection**: Not implemented
- âŒ **Training Data Auditing**: Not implemented
- âš ï¸ **Statistical Analysis**: Basic performance monitoring only

#### Threat: Model Weight Tampering
- **Description**: Direct modification of trained model parameters
- **Attack Vector**: File system access, insider threats, supply chain attacks
- **Impact**: HIGH - Arbitrary behavior modification
- **Likelihood**: LOW - Requires privileged access
- **Evidence**: SAST analysis flagged `torch.load()` usage, but **NOTE**: In modern PyTorch versions (>=1.13.0), `weights_only=True` is the default, automatically mitigating pickle deserialization vulnerabilities

**Security Evolution Note**: The Bandit SAST tool flags `torch.load()` as potentially unsafe due to pickle deserialization concerns in older PyTorch versions. However, PyTorch 1.13.0+ defaults to `weights_only=True`, making the flagged code secure without explicit parameter specification.

---

### ğŸš« **R - Repudiation**

#### Threat: Lack of Decision Auditability
- **Description**: Inability to trace and verify model decisions
- **Attack Vector**: Plausible deniability of malicious classifications
- **Impact**: MEDIUM - Trust and accountability issues
- **Likelihood**: HIGH - No auditing mechanisms implemented
- **Evidence**: No logging of prediction confidence, intermediate activations, or decision paths

**Specific Scenarios:**
1. **Decision Denial**: Users denying incorrect classifications were legitimate inputs
2. **Model Behavior Denial**: Developers denying knowledge of backdoor behaviors
3. **Training Process Denial**: Claims of unintentional bias or poisoning

**Mitigations:**
- âŒ **Decision Logging**: Not implemented
- âŒ **Model Explainability**: Not implemented
- âŒ **Audit Trails**: Not implemented
- âš ï¸ **Performance Metrics**: Basic accuracy logging only

---

### ğŸ“Š **I - Information Disclosure**

#### Threat: Model Extraction and Intellectual Property Theft
- **Description**: Unauthorized access to model architecture or parameters
- **Attack Vector**: Model inversion, parameter extraction, architecture reverse engineering
- **Impact**: MEDIUM - Competitive advantage loss, privacy violations
- **Likelihood**: MEDIUM - Possible through query-based attacks
- **Evidence**: Model weights stored in unencrypted .pth files

**Specific Scenarios:**
1. **Weight Extraction**: Direct access to model parameter files
2. **Architecture Inference**: Reverse engineering through input-output analysis
3. **Training Data Inference**: Membership inference attacks on training samples

**Mitigations:**
- âŒ **Model Encryption**: Not implemented
- âŒ **Access Controls**: Basic file system permissions only
- âŒ **Query Limiting**: Not implemented
- âŒ **Differential Privacy**: Not implemented

#### Threat: Training Data Exposure
- **Description**: Leaking information about individual training samples
- **Attack Vector**: Model inversion, membership inference attacks
- **Impact**: LOW - MNIST is public dataset
- **Likelihood**: LOW - Limited sensitive information in MNIST
- **Evidence**: No privacy protection mechanisms implemented

---

### ğŸ’¥ **D - Denial of Service**

#### Threat: Adversarial Input-Based DoS
- **Description**: Inputs designed to cause excessive resource consumption
- **Attack Vector**: Adversarial examples causing computational spikes
- **Impact**: HIGH - System unavailability, resource exhaustion
- **Likelihood**: MEDIUM - Possible with carefully crafted inputs
- **Evidence**: No input validation or rate limiting implemented

**Specific Scenarios:**
1. **Computational Bombing**: Inputs causing excessive GPU/CPU usage
2. **Memory Exhaustion**: Large batch sizes causing OOM errors
3. **Inference Loops**: Inputs causing infinite processing loops

**Mitigations:**
- âŒ **Input Size Limiting**: Not implemented
- âŒ **Rate Limiting**: Not implemented
- âŒ **Resource Monitoring**: Not implemented
- âš ï¸ **Timeout Mechanisms**: Basic batch processing only

#### Threat: Training Process Disruption
- **Description**: Attacks targeting the training infrastructure
- **Attack Vector**: Resource exhaustion, malformed training data
- **Impact**: HIGH - Training failure, model corruption
- **Likelihood**: LOW - Limited exposure in research environment
- **Evidence**: No robust error handling in training pipeline

---

### â¬†ï¸ **E - Elevation of Privilege**

#### Threat: Model-Based Privilege Escalation
- **Description**: Using model predictions to bypass security controls
- **Attack Vector**: Adversarial examples causing incorrect high-confidence predictions
- **Impact**: HIGH - Unauthorized access to protected resources
- **Likelihood**: MEDIUM - Depends on downstream system design
- **Evidence**: No confidence thresholding or uncertainty quantification

**Specific Scenarios:**
1. **Authentication Bypass**: High-confidence misclassifications in security applications
2. **Access Control Subversion**: Model predictions used for authorization decisions
3. **Privilege Expansion**: Leveraging model trust for unauthorized actions

**Mitigations:**
- âŒ **Confidence Thresholding**: Not implemented
- âŒ **Multi-Factor Verification**: Not implemented
- âŒ **Uncertainty Quantification**: Not implemented
- âŒ **Human-in-the-Loop**: Not implemented

---

## Risk Assessment Matrix

| Threat Category | Specific Threat | Likelihood | Impact | Risk Level | Mitigation Status |
|-----------------|-----------------|------------|---------|------------|-------------------|
| **Spoofing** | Adversarial Examples | HIGH | HIGH | **CRITICAL** | Partially Mitigated |
| **Spoofing** | Model Identity Spoofing | MEDIUM | CRITICAL | **HIGH** | Not Mitigated |
| **Tampering** | Data Poisoning | MEDIUM | HIGH | **HIGH** | Not Mitigated |
| **Tampering** | Weight Tampering | LOW | HIGH | **MEDIUM** | Not Mitigated |
| **Repudiation** | Decision Auditability | HIGH | MEDIUM | **MEDIUM** | Not Mitigated |
| **Information** | Model Extraction | MEDIUM | MEDIUM | **MEDIUM** | Not Mitigated |
| **Information** | Data Exposure | LOW | LOW | **LOW** | N/A (Public Data) |
| **DoS** | Adversarial DoS | MEDIUM | HIGH | **HIGH** | Not Mitigated |
| **DoS** | Training Disruption | LOW | HIGH | **MEDIUM** | Not Mitigated |
| **Privilege** | Model-Based Escalation | MEDIUM | HIGH | **HIGH** | Not Mitigated |

---

## Security Recommendations

### Immediate Actions (Critical Risk)
1. **Implement Secure Model Loading**
   - Use `torch.load(file, weights_only=True)`
   - Implement model file integrity verification
   - Add cryptographic signatures for model files

2. **Enhance Adversarial Robustness**
   - Deploy adversarially trained models in production
   - Implement adversarial example detection
   - Add input preprocessing defenses

### Short-term Improvements (High Risk)
1. **Training Data Security**
   - Implement data integrity verification
   - Add anomaly detection for poisoned samples
   - Establish secure data pipelines

2. **Denial of Service Protection**
   - Add input validation and sanitization
   - Implement rate limiting and resource monitoring
   - Create timeout mechanisms for inference

### Medium-term Enhancements (Medium Risk)
1. **Auditability and Transparency**
   - Implement decision logging and audit trails
   - Add model explainability features
   - Create confidence scoring systems

2. **Information Protection**
   - Add access controls and encryption
   - Implement query limiting and monitoring
   - Consider differential privacy techniques

### Long-term Strategic Improvements
1. **Comprehensive Security Framework**
   - Develop ML-specific security policies
   - Implement continuous security monitoring
   - Establish incident response procedures

2. **Advanced Defense Mechanisms**
   - Deploy certified defense methods
   - Implement ensemble-based robustness
   - Add formal verification techniques

---

## Security Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Security Controls Layer                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Input Layer   â”‚   Model Layer   â”‚     Output Layer        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Input Valid.  â”‚ â€¢ Adv. Training â”‚ â€¢ Confidence Scoring    â”‚
â”‚ â€¢ Adversarial   â”‚ â€¢ Model Encrypt â”‚ â€¢ Decision Logging      â”‚
â”‚   Detection     â”‚ â€¢ Access Controlâ”‚ â€¢ Audit Trails          â”‚
â”‚ â€¢ Preprocessing â”‚ â€¢ Integrity Ver â”‚ â€¢ Rate Limiting         â”‚
â”‚ â€¢ Rate Limiting â”‚ â€¢ Secure Loadingâ”‚ â€¢ Output Validation     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Conclusion

This STRIDE analysis reveals significant security vulnerabilities in the MNIST CNN system, with **Critical** and **High** risk threats primarily related to adversarial attacks, data poisoning, and model tampering. While adversarial training provides some protection against gradient-based attacks, comprehensive security requires a multi-layered approach addressing all threat categories.

**Priority Actions:**
1. Fix unsafe model loading (immediate security vulnerability)
2. Implement comprehensive input validation and adversarial detection
3. Establish data integrity verification for training pipelines
4. Add auditability and monitoring capabilities

The implemented adversarial training demonstrates the effectiveness of proactive security measures, improving adversarial robustness from 43.65% to 95.02% accuracy. However, this addresses only one aspect of the broader security landscape identified through systematic threat modeling.

---

**Threat Model Version**: 1.0  
**Last Updated**: November 24, 2025  
**Next Review**: Quarterly or after significant system changes